# Fairness-Aware Structured Pruning in Transformers (FASP)
Welcome to the official repository for the AAAI 2024 paper “Fairness-Aware Structured Pruning in Transformers”. We present a novel method to prune the attention heads that negatively impact fairness in transformer-based models while retaining most of the language modeling ability.

<div style="text-align: center">
<img src="FASP_figure.png" width="400">
<p style="text-align: center;"> </p>
</div>

## Running the experiments
Get started with the Colab tutorial, `FASP_AAAI24_reproducibility.ipynb`, which guides you through the process of downloading the models, performing preprocessing steps, and creating the scripts required to run the experiments. 

## Citation
```
@article{zayed2023fairness,
  title={Fairness-Aware Structured Pruning in Transformers},
  author={Zayed, Abdelrahman and Mordido, Goncalo and Shabanian, Samira and Baldini, Ioana and Chandar, Sarath},
  journal={arXiv preprint arXiv:2312.15398},
  year={2023}
}


