# pruning-and-fairness-in-LLMs
Welcome to the official repository for the AAAI 2024 submission “Fairness-Aware Structured Pruning in Transformers”. We present a novel method to prune the attention heads that negatively impact fairness in transformer-based models while retaining most of the language modeling ability.

## How it works
Get started with our colab tutorial, [`FASP_AAAI24_reproducibility.ipynb`](https://colab.research.google.com/drive/1aRs867Y7rAuBLj8bc9Rwm-paKiHFac8F?authuser=1), which provides a comprehensive guide that takes you through the process of downloading the dataset, performing preprocessing steps, and creating the essential scripts required to run the experiments. 


